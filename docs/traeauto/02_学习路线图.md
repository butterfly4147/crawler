# 分布式爬虫项目学习路线图

## 学习目标

基于你2年Go语言游戏服务器开发经验，采用**自顶向下**的软件工程方式，系统掌握这个分布式爬虫项目的架构设计、核心实现和扩展能力。

## 前置知识检查

### 你已具备的技能 ✅
- Go语言基础语法和并发编程
- 分布式系统基本概念
- 网络编程和RPC通信
- 数据库操作和ORM使用
- 微服务架构理解

### 需要补充的知识点 📚
- etcd的使用和分布式协调
- gRPC和Protocol Buffers
- 网页抓取和HTML解析
- 正则表达式和数据提取
- Docker容器化部署

## 第一阶段：架构理解与环境搭建 (2-3天)

### Day 1: 宏观架构认知

#### 上午 (3小时)
**学习目标**: 建立项目的整体认知

**学习内容**:
1. 阅读 `docs/traeauto/00_项目架构总览.md`
2. 理解Master-Worker分布式架构模式
3. 对比游戏服务器架构，找出相似点和差异

**实践任务**:
```bash
# 1. 克隆项目并查看目录结构
tree -L 3 .

# 2. 查看主要配置文件
cat config.toml
cat docker-compose.yml

# 3. 查看项目依赖
go mod graph | head -20
```

**关键理解点**:
- Master节点的选举机制 ≈ 游戏主服务器选举
- Worker节点的任务执行 ≈ 游戏房间服务器
- etcd的作用 ≈ 游戏服务器注册中心

#### 下午 (3小时)
**学习目标**: 理解项目启动流程

**学习内容**:
1. 分析 `main.go` → `cmd/cmd.go` 启动链路
2. 理解 `cmd/master/master.go` 和 `cmd/worker/worker.go`
3. 掌握配置文件结构和参数含义

**实践任务**:
```bash
# 1. 编译项目
go build -o crawler.exe .

# 2. 查看命令行帮助
./crawler.exe --help
./crawler.exe master --help
./crawler.exe worker --help

# 3. 分析启动参数
# Master: --id --http --grpc --pprof
# Worker: --id --http --grpc
```

**代码阅读顺序**:
1. `main.go` (10行) - 项目入口
2. `cmd/cmd.go` (50行) - 命令定义
3. `cmd/master/master.go` (前100行) - Master启动逻辑
4. `cmd/worker/worker.go` (前100行) - Worker启动逻辑

### Day 2: 环境搭建与运行

#### 上午 (3小时)
**学习目标**: 搭建完整的运行环境

**环境准备**:
```bash
# 1. 安装MySQL (如果未安装)
# 下载MySQL 5.7+，设置root密码: 123456

# 2. 创建数据库
mysql -u root -p123456 -e "CREATE DATABASE IF NOT EXISTS crawler DEFAULT CHARSET utf8;"

# 3. 安装etcd
# 下载etcd 3.5+，解压到PATH目录
```

**Docker方式 (推荐)**:
```bash
# 使用Docker Compose一键启动
docker-compose up -d

# 查看服务状态
docker-compose ps
```

#### 下午 (3小时)
**学习目标**: 验证系统运行

**启动服务**:
```bash
# 方式1: 手动启动 (推荐学习时使用)
# 终端1: 启动etcd
etcd --data-dir=default.etcd --listen-client-urls=http://127.0.0.1:2379 --advertise-client-urls=http://127.0.0.1:2379

# 终端2: 启动Master
./crawler.exe master --id=2 --http=:8082 --grpc=:9092 --pprof=:9982

# 终端3: 启动Worker
./crawler.exe worker --id=1 --http=:8080 --grpc=:9090

# 方式2: Docker Compose
docker-compose up
```

**验证功能**:
```bash
# 1. 检查服务状态
curl http://localhost:8082/health  # Master健康检查
curl http://localhost:8080/health  # Worker健康检查

# 2. 添加爬虫任务
curl -X POST http://localhost:8082/crawler/resource \
  -H "Content-Type: application/json" \
  -d '{"name": "example_baidu_home"}'

# 3. 查看数据库结果
mysql -u root -p123456 -e "USE crawler; SHOW TABLES; SELECT * FROM example_baidu_home LIMIT 5;"
```

### Day 3: 分布式框架理解

#### 上午 (3小时)
**学习目标**: 深入理解分布式协调机制

**学习内容**:
1. etcd的基本概念和使用
2. 服务注册与发现机制
3. Leader选举算法原理

**实践任务**:
```bash
# 1. etcd基本操作
etcdctl put /test/key "hello world"
etcdctl get /test/key
etcdctl watch /test/key

# 2. 观察服务注册
etcdctl get --prefix /micro/registry/

# 3. 观察选举过程
etcdctl get --prefix /resources/election/
```

**代码阅读**:
- `master/master.go` 中的 `Campaign()` 方法
- 理解 `concurrency.Election` 的使用
- 分析选举失败时的转发机制

#### 下午 (3小时)
**学习目标**: 理解任务分配机制

**学习内容**:
1. 负载均衡算法实现
2. 资源分配和监听机制
3. 节点状态管理

**代码阅读顺序**:
1. `master/master.go` - `Assign()` 方法 (负载均衡)
2. `master/master.go` - `AddResource()` 方法 (任务分配)
3. `engine/schedule.go` - `watchResource()` 方法 (资源监听)

**与游戏服务器对比**:
- 负载均衡 ≈ 玩家分配到游戏房间
- 资源监听 ≈ 游戏服务器监听配置变化
- 节点管理 ≈ 游戏服务器集群管理

## 第二阶段：核心模块深入 (4-5天)

### Day 4: Master模块深度分析

#### 上午 (3小时)
**学习目标**: 掌握Master节点的完整实现

**代码阅读计划**:
```go
// 1. 数据结构理解 (30分钟)
type Master struct {
    ID         string                        // 节点标识
    workNodes  map[string]*NodeSpec          // Worker节点映射
    resources  map[string]*ResourceSpec      // 任务资源映射
    // ... 其他字段
}

// 2. 选举机制 (60分钟)
func (m *Master) Campaign()     // 参与选举
func (m *Master) elect()        // 选举处理
func (m *Master) BecomeLeader() // 成为Leader

// 3. 任务管理 (90分钟)
func (m *Master) AddResource()    // 添加任务
func (m *Master) DeleteResource() // 删除任务
func (m *Master) Assign()         // 任务分配
```

**实践任务**:
1. 在 `Campaign()` 方法中添加日志，观察选举过程
2. 修改负载均衡算法，实现轮询策略
3. 添加节点健康检查机制

#### 下午 (3小时)
**学习目标**: 理解RPC服务实现

**学习内容**:
1. gRPC服务定义和实现
2. HTTP网关的映射机制
3. 请求转发和错误处理

**代码阅读**:
- `proto/crawler/crawler.proto` - 服务定义
- `proto/crawler/crawler.pb.go` - 生成的代码
- `proto/crawler/crawler.pb.gw.go` - HTTP网关

**实践任务**:
```bash
# 1. 重新生成protobuf代码
protoc --go_out=. --go-grpc_out=. proto/crawler/crawler.proto

# 2. 测试gRPC接口
grpcurl -plaintext localhost:9092 list
grpcurl -plaintext localhost:9092 crawler.CrawlerMaster/AddResource

# 3. 测试HTTP接口
curl -X POST http://localhost:8082/crawler/resource -d '{"name":"test"}'
```

### Day 5: Engine模块深度分析

#### 上午 (3小时)
**学习目标**: 掌握爬虫引擎的调度机制

**代码阅读计划**:
```go
// 1. 调度器接口 (30分钟)
type Scheduler interface {
    Schedule()                    // 调度逻辑
    Push(...*spider.Request)      // 请求入队
    Pull() *spider.Request        // 请求出队
}

// 2. 调度实现 (90分钟)
type Schedule struct {
    requestCh   chan *spider.Request  // 请求通道
    workerCh    chan *spider.Request  // 工作通道
    priReqQueue []*spider.Request     // 优先级队列
    reqQueue    []*spider.Request     // 普通队列
}

// 3. 爬虫引擎 (60分钟)
type Crawler struct {
    out         chan spider.ParseResult  // 结果输出
    Visited     map[string]bool          // 去重映射
    failures    map[string]*spider.Request // 失败重试
}
```

**关键理解点**:
- 调度器 ≈ 游戏服务器中的任务调度器
- 优先级队列 ≈ 游戏中的优先级任务
- 并发控制 ≈ 游戏中的玩家并发处理

#### 下午 (3小时)
**学习目标**: 理解并发工作模型

**代码阅读**:
- `engine/schedule.go` - `CreateWork()` 方法
- 理解goroutine池的管理
- 分析去重和重试机制

**实践任务**:
1. 添加并发度控制参数
2. 实现更智能的重试策略
3. 添加任务执行统计

**性能优化思考**:
```go
// 可以优化的点:
// 1. 使用sync.Pool复用Request对象
// 2. 批量处理请求减少锁竞争
// 3. 使用布隆过滤器优化去重
// 4. 实现分布式去重机制
```

### Day 6: Spider模块深度分析

#### 上午 (3小时)
**学习目标**: 理解任务定义和解析规则

**代码阅读计划**:
```go
// 1. 任务结构 (45分钟)
type Task struct {
    Visited     map[string]bool  // 已访问URL
    Rule        RuleTree         // 解析规则树
    Options                      // 任务选项
}

// 2. 规则树 (90分钟)
type RuleTree struct {
    Root  func() ([]*Request, error)  // 入口函数
    Trunk map[string]*Rule            // 规则映射
}

type Rule struct {
    ItemFields []string                           // 数据字段
    ParseFunc  func(*Context) (ParseResult, error) // 解析函数
}

// 3. 请求生命周期 (45分钟)
type Request struct {
    URL      string  // 请求URL
    Depth    int64   // 爬取深度
    Priority int64   // 优先级
    RuleName string  // 规则名称
}
```

**实践任务**:
1. 创建一个简单的爬虫任务
2. 实现自定义的解析规则
3. 测试深度限制和去重机制

#### 下午 (3小时)
**学习目标**: 学习具体的解析实现

**代码阅读**:
- `parse/minimal/minimal.go` - 最简单的示例
- `parse/doubanbook/book.go` - 豆瓣图书爬虫
- `parse/doubangroupjs/groupjs.go` - JavaScript解析

**实践任务**:
```go
// 创建自定义爬虫任务
var CustomTask = &spider.Task{
    Rule: spider.RuleTree{
        Root: func() ([]*spider.Request, error) {
            return []*spider.Request{
                {
                    URL:      "https://example.com",
                    Method:   "GET",
                    RuleName: "parse_home",
                },
            }, nil
        },
        Trunk: map[string]*spider.Rule{
            "parse_home": {
                ItemFields: []string{"title", "content"},
                ParseFunc: func(ctx *spider.Context) (spider.ParseResult, error) {
                    // 实现解析逻辑
                    return spider.ParseResult{}, nil
                },
            },
        },
    },
}
```

### Day 7-8: 存储与扩展模块

#### Day 7: 存储模块分析
**学习内容**:
- `storage/storage.go` - 存储接口定义
- `storage/sqlstorage/` - SQL存储实现
- `sqldb/` - 数据库操作封装

**实践任务**:
1. 实现Redis存储插件
2. 实现文件存储插件
3. 添加数据验证和清洗

#### Day 8: 扩展模块分析
**学习内容**:
- `collect/collect.go` - 抓取器实现
- `limiter/limiter.go` - 限流器实现
- `proxy/proxy.go` - 代理管理

**实践任务**:
1. 实现浏览器抓取器
2. 添加代理轮换机制
3. 实现智能限流策略

## 第三阶段：分布式特性掌握 (3-4天)

### Day 9: 服务发现与通信

#### 上午 (3小时)
**学习目标**: 深入理解go-micro框架

**学习内容**:
1. go-micro的服务注册机制
2. gRPC客户端和服务端实现
3. 负载均衡和故障转移

**代码阅读**:
- 分析服务注册的完整流程
- 理解客户端负载均衡实现
- 掌握健康检查机制

#### 下午 (3小时)
**学习目标**: etcd深度使用

**实践任务**:
```bash
# 1. 监听服务变化
etcdctl watch --prefix /micro/registry/

# 2. 模拟节点故障
# 停止一个Worker，观察Master的反应

# 3. 测试选举机制
# 停止Master，观察新Leader的产生
```

### Day 10: 高可用与容错

#### 学习内容:
1. Leader选举的详细实现
2. 故障检测和恢复机制
3. 数据一致性保证

**实践任务**:
1. 实现Master节点的优雅关闭
2. 添加Worker节点的健康检查
3. 实现任务的故障转移

### Day 11: 监控与运维

#### 学习内容:
1. 添加Prometheus监控指标
2. 实现日志聚合和分析
3. 性能调优和瓶颈分析

**实践任务**:
1. 集成Prometheus监控
2. 添加关键指标统计
3. 实现告警机制

## 第四阶段：实战项目与扩展 (3-5天)

### Day 12-13: 实战项目

**项目目标**: 实现一个完整的新闻爬虫系统

**功能要求**:
1. 支持多个新闻网站
2. 实现增量更新
3. 添加内容去重
4. 支持图片下载
5. 实现数据清洗

**技术要求**:
1. 使用浏览器抓取JS渲染页面
2. 实现分布式去重
3. 添加监控和告警
4. 支持动态配置更新

### Day 14-15: 性能优化

**优化目标**:
1. 提升抓取速度
2. 降低内存使用
3. 优化数据库写入
4. 实现智能调度

**优化方案**:
```go
// 1. 使用对象池
var requestPool = sync.Pool{
    New: func() interface{} {
        return &spider.Request{}
    },
}

// 2. 批量数据库操作
func (s *SQLStorage) BatchSave(items []interface{}) error {
    // 批量插入实现
}

// 3. 布隆过滤器去重
type BloomFilter struct {
    bits []bool
    hash []hash.Hash
}

// 4. 分布式队列
type DistributedQueue struct {
    redis *redis.Client
}
```

### Day 16: 部署与运维

**学习内容**:
1. Kubernetes部署配置
2. Docker镜像优化
3. CI/CD流水线搭建
4. 生产环境监控

**实践任务**:
1. 编写Kubernetes部署文件
2. 实现滚动更新
3. 配置日志收集
4. 设置监控告警

## 学习资源推荐

### 官方文档
- [go-micro官方文档](https://go-micro.dev/)
- [etcd官方文档](https://etcd.io/docs/)
- [gRPC Go教程](https://grpc.io/docs/languages/go/)

### 相关书籍
- 《分布式系统概念与设计》
- 《Go语言高级编程》
- 《微服务架构设计模式》

### 开源项目
- [Colly爬虫框架](https://github.com/gocolly/colly)
- [Scrapy分布式爬虫](https://github.com/scrapy/scrapy)
- [go-micro示例项目](https://github.com/go-micro/examples)

## 学习检查点

### 第一阶段检查 ✅
- [ ] 能够独立搭建运行环境
- [ ] 理解Master-Worker架构模式
- [ ] 掌握etcd基本使用
- [ ] 能够添加和执行爬虫任务

### 第二阶段检查 ✅
- [ ] 理解Master选举和任务分配机制
- [ ] 掌握Engine调度和并发控制
- [ ] 能够编写自定义爬虫规则
- [ ] 理解存储和扩展机制

### 第三阶段检查 ✅
- [ ] 掌握分布式协调和通信
- [ ] 理解高可用和容错设计
- [ ] 能够进行性能监控和调优
- [ ] 具备运维和故障排查能力

### 第四阶段检查 ✅
- [ ] 能够独立设计和实现爬虫系统
- [ ] 掌握性能优化技巧
- [ ] 具备生产环境部署能力
- [ ] 能够扩展和定制功能

## 进阶方向

### 技术深度
1. **分布式系统**: 深入学习一致性算法、分布式事务
2. **高性能编程**: Go语言性能优化、内存管理
3. **云原生**: Kubernetes、Service Mesh、Serverless

### 业务扩展
1. **大数据处理**: 集成Kafka、Spark、Flink
2. **机器学习**: 内容分类、智能推荐、反爬虫
3. **实时计算**: 流式处理、实时监控、动态调度

### 架构演进
1. **微服务治理**: 服务网格、配置中心、链路追踪
2. **多云部署**: 混合云、容灾备份、弹性伸缩
3. **边缘计算**: CDN集成、边缘节点、就近处理

通过这个系统的学习路线，你将能够完全掌握这个分布式爬虫项目，并具备设计和实现类似分布式系统的能力。结合你的游戏服务器经验，相信你能够快速理解和掌握这些概念。